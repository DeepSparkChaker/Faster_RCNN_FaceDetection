{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fix many bugs in our cloned reprository i modified the code: \n",
    "# Common Methods\n",
    "\n",
    "This module includes a lot of handy methods that is used in training and evaluation, it has a class for configuration and some image methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "import math\n",
    "import cv2\n",
    "import copy\n",
    "import random\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Minimum tensorflow version\n",
    "MINIMUM_TF_VERSION = 1, 12, 0\n",
    "\n",
    "# Restrict tensortlow to only use the one GPU\n",
    "def setup_gpu(gpu_id):\n",
    "    if tf_version_ok((2, 0, 0)):\n",
    "        if gpu_id == 'cpu' or gpu_id == -1:\n",
    "            tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "            return\n",
    "\n",
    "        # Get all gpus\n",
    "        gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            # Restrict TensorFlow to only use the first GPU.\n",
    "            try:\n",
    "                # Currently, memory growth needs to be the same across GPUs.\n",
    "                for gpu in gpus:\n",
    "                    #tf.config.experimental.set_virtual_device_configuration(gpu, [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "                    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "                # Use only the selcted gpu.\n",
    "                tf.config.experimental.set_visible_devices(gpus[gpu_id], 'GPU')\n",
    "            except RuntimeError as e:\n",
    "                # Visible devices must be set before GPUs have been initialized.\n",
    "                print(e)\n",
    "\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    else:\n",
    "        import os\n",
    "        if gpu_id == 'cpu' or gpu_id == -1:\n",
    "            os.environ['CUDA_VISIBLE_DEVICES'] = \"\"\n",
    "            return\n",
    "\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = str(gpu_id)\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "        config.gpu_options.allow_growth = True\n",
    "        tf.keras.backend.set_session(tf.Session(config=config))\n",
    "\n",
    "# Get the Tensorflow version\n",
    "def tf_version():\n",
    "    return tuple(map(int, tf.version.VERSION.split('-')[0].split('.')))\n",
    "\n",
    "# Check if the current Tensorflow version is higher than the minimum version\n",
    "def tf_version_ok(minimum_tf_version=MINIMUM_TF_VERSION):\n",
    "    return tf_version() >= minimum_tf_version\n",
    "\n",
    "# This class is used for configuration\n",
    "class Config:\n",
    "\n",
    "    # Initializes the class\n",
    "    def __init__(self):\n",
    "\n",
    "        # Print the process or not\n",
    "        self.verbose = True\n",
    "\n",
    "        # Settings for data augmentation\n",
    "        self.use_horizontal_flips = False\n",
    "        self.use_vertical_flips = False\n",
    "        self.rot_90 = False\n",
    "\n",
    "        # Anchor box scales\n",
    "        # Note that if im_size is smaller, anchor_box_scales should be scaled\n",
    "        # Original anchor_box_scales in the paper is [128, 256, 512]\n",
    "        self.anchor_box_scales = [64, 128, 256] \n",
    "\n",
    "        # Anchor box ratios\n",
    "        self.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n",
    "\n",
    "        # Size to resize the smallest side of the image\n",
    "        # Original setting in paper is 600. Set to 300 in here to save training time\n",
    "        self.im_size = 300\n",
    "\n",
    "        # image channel-wise mean to subtract\n",
    "        self.img_channel_mean = [103.939, 116.779, 123.68]\n",
    "        self.img_scaling_factor = 1.0\n",
    "\n",
    "        # number of ROIs at once\n",
    "        self.num_rois = 4\n",
    "\n",
    "        # stride at the RPN (this depends on the network configuration)\n",
    "        self.rpn_stride = 16\n",
    "\n",
    "        # scaling the stdev\n",
    "        self.std_scaling = 4.0\n",
    "        self.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
    "\n",
    "        # overlaps for RPN\n",
    "        self.rpn_min_overlap = 0.3\n",
    "        self.rpn_max_overlap = 0.7\n",
    "\n",
    "        # overlaps for classifier ROIs\n",
    "        self.classifier_min_overlap = 0.1\n",
    "        self.classifier_max_overlap = 0.5\n",
    "\n",
    "        # Loss function settings\n",
    "        self.lambda_rpn_regr = 1.0\n",
    "        self.lambda_rpn_class = 1.0\n",
    "        self.lambda_cls_regr = 1.0\n",
    "        self.lambda_cls_class = 1.0\n",
    "        self.epsilon = 1e-4\n",
    "\n",
    "        # Paths\n",
    "        self.annotations_file_path = None\n",
    "        self.pretrained_model_path = None\n",
    "        self.model_path = None\n",
    "        self.records_path = None\n",
    "\n",
    "# Parse the data from annotation file\n",
    "def get_data(config):\n",
    "\t\n",
    "    # Make sure that there is configurations\n",
    "    if config == None:\n",
    "        config = Config()\n",
    "\n",
    "    # Variables\n",
    "    found_bg = False\n",
    "    all_imgs = {}\n",
    "    classes_count = {}\n",
    "    class_mapping = {}\n",
    "    visualise = True\n",
    "    i = 1\n",
    "\n",
    "    with open(config.annotations_file_path,'r') as f:\n",
    "\n",
    "        print('Parsing annotation file')\n",
    "\n",
    "        for line in f:\n",
    "\n",
    "            # Print process\n",
    "            sys.stdout.write('\\r'+'idx=' + str(i))\n",
    "            i += 1\n",
    "\n",
    "            line_split = line.strip().split(',')\n",
    "\n",
    "            # Make sure the info saved in annotation file matching the format (path_filename, x1, y1, x2, y2, class_name)\n",
    "            (filename,x1,y1,x2,y2,class_name) = line_split\n",
    "\n",
    "            if class_name not in classes_count:\n",
    "                classes_count[class_name] = 1\n",
    "            else:\n",
    "                classes_count[class_name] += 1\n",
    "\n",
    "            if class_name not in class_mapping:\n",
    "                if class_name == 'bg' and found_bg == False:\n",
    "                    print('Found class name with special name bg. Will be treated as a background region (this is usually for hard negative mining).')\n",
    "                    found_bg = True\n",
    "                class_mapping[class_name] = len(class_mapping)\n",
    "\n",
    "            if filename not in all_imgs:\n",
    "                all_imgs[filename] = {}\n",
    "\n",
    "                img = cv2.imread(filename)\n",
    "                (rows,cols) = img.shape[:2]\n",
    "                all_imgs[filename]['filepath'] = filename\n",
    "                all_imgs[filename]['width'] = cols\n",
    "                all_imgs[filename]['height'] = rows\n",
    "                all_imgs[filename]['bboxes'] = []\n",
    "\n",
    "            all_imgs[filename]['bboxes'].append({'class': class_name, 'x1': int(x1), 'x2': int(x2), 'y1': int(y1), 'y2': int(y2)})\n",
    "\n",
    "\n",
    "        all_data = []\n",
    "        for key in all_imgs:\n",
    "            all_data.append(all_imgs[key])\n",
    "\n",
    "        # make sure the bg class is last in the list\n",
    "        if found_bg:\n",
    "            if class_mapping['bg'] != len(class_mapping) - 1:\n",
    "                key_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping)-1][0]\n",
    "                val_to_switch = class_mapping['bg']\n",
    "                class_mapping['bg'] = len(class_mapping) - 1\n",
    "                class_mapping[key_to_switch] = val_to_switch\n",
    "\n",
    "        return all_data, classes_count, class_mapping\n",
    "\n",
    "# Loss function for rpn regression\n",
    "def rpn_loss_regr(num_anchors, config=None):\n",
    "    \n",
    "    # Make sure that there is configurations\n",
    "    if config == None:\n",
    "        config = Config()\n",
    "\n",
    "    def rpn_loss_regr_fixed_num(y_true, y_pred):\n",
    "\n",
    "        # x is the difference between true value and predicted vaue\n",
    "        x = y_true[:, :, :, 4 * num_anchors:] - y_pred\n",
    "\n",
    "        # absolute value of x\n",
    "        x_abs = keras.backend.abs(x)\n",
    "\n",
    "        # If x_abs <= 1.0, x_bool = 1\n",
    "        x_bool = keras.backend.cast(keras.backend.less_equal(x_abs, 1.0), tf.float32)\n",
    "\n",
    "        return config.lambda_rpn_regr * keras.backend.sum(\n",
    "            y_true[:, :, :, :4 * num_anchors] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / keras.backend.sum(config.epsilon + y_true[:, :, :, :4 * num_anchors])\n",
    "\n",
    "    return rpn_loss_regr_fixed_num\n",
    "\n",
    "# Loss function for rpn classification\n",
    "def rpn_loss_cls(num_anchors, config=None):\n",
    "    \n",
    "    # Make sure that there is configurations\n",
    "    if config == None:\n",
    "        config = Config()\n",
    "\n",
    "    def rpn_loss_cls_fixed_num(y_true, y_pred):\n",
    "        return config.lambda_rpn_class * keras.backend.sum(y_true[:, :, :, :num_anchors] * keras.backend.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, num_anchors:])) / keras.backend.sum(config.epsilon + y_true[:, :, :, :num_anchors])\n",
    "\n",
    "    # Return a function\n",
    "    return rpn_loss_cls_fixed_num\n",
    "\n",
    "# Loss function for rpn regression\n",
    "def class_loss_regr(num_classes, config=None):\n",
    "\n",
    "    # Make sure that there is configurations\n",
    "    if config == None:\n",
    "        config = Config()\n",
    "\n",
    "    def class_loss_regr_fixed_num(y_true, y_pred):\n",
    "        x = y_true[:, :, 4*num_classes:] - y_pred\n",
    "        x_abs = keras.backend.abs(x)\n",
    "        x_bool = keras.backend.cast(keras.backend.less_equal(x_abs, 1.0), 'float32')\n",
    "        return config.lambda_cls_regr * keras.backend.sum(y_true[:, :, :4*num_classes] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / keras.backend.sum(config.epsilon + y_true[:, :, :4*num_classes])\n",
    "    \n",
    "    # Return a function\n",
    "    return class_loss_regr_fixed_num\n",
    "\n",
    "# Loss function for classification y_true, y_pred,\n",
    "def class_loss_cls(config=None):\n",
    "\n",
    "    # Make sure that there is configurations\n",
    "    if config == None:\n",
    "        config = Config()\n",
    "\n",
    "    def class_loss_cls_fixed_num(y_true, y_pred):\n",
    "        return config.lambda_cls_class * keras.backend.mean(keras.losses.categorical_crossentropy(y_true[0, :, :], y_pred[0, :, :]))\n",
    "\n",
    "    # Return a function\n",
    "    return class_loss_cls_fixed_num\n",
    "    \n",
    "# Get image output length\n",
    "def get_img_output_length(width, height):\n",
    "    \n",
    "    def get_output_length(input_length):\n",
    "        return input_length//16\n",
    "\n",
    "    # Return output length for width and height\n",
    "    return get_output_length(width), get_output_length(height)\n",
    "\n",
    "# Get a new image size\n",
    "def get_new_img_size(width, height, img_min_side=300):\n",
    "\t\n",
    "    if width <= height:\n",
    "        f = float(img_min_side) / width\n",
    "        resized_height = int(f * height)\n",
    "        resized_width = img_min_side\n",
    "    else:\n",
    "        f = float(img_min_side) / height\n",
    "        resized_width = int(f * width)\n",
    "        resized_height = img_min_side\n",
    "\n",
    "    # Return resized width and height\n",
    "    return resized_width, resized_height\n",
    "\n",
    "# Augument\n",
    "def augment(img_data, config, augment=True):\n",
    "\tassert 'filepath' in img_data\n",
    "\tassert 'bboxes' in img_data\n",
    "\tassert 'width' in img_data\n",
    "\tassert 'height' in img_data\n",
    "\n",
    "\timg_data_aug = copy.deepcopy(img_data)\n",
    "\n",
    "\timg = cv2.imread( img_data_aug['filepath'])\n",
    "\n",
    "\tif augment:\n",
    "\t\trows, cols = img.shape[:2]\n",
    "\n",
    "\t\tif config.use_horizontal_flips and np.random.randint(0, 2) == 0:\n",
    "\t\t\timg = cv2.flip(img, 1)\n",
    "\t\t\tfor bbox in img_data_aug['bboxes']:\n",
    "\t\t\t\tx1 = bbox['x1']\n",
    "\t\t\t\tx2 = bbox['x2']\n",
    "\t\t\t\tbbox['x2'] = cols - x1\n",
    "\t\t\t\tbbox['x1'] = cols - x2\n",
    "\n",
    "\t\tif config.use_vertical_flips and np.random.randint(0, 2) == 0:\n",
    "\t\t\timg = cv2.flip(img, 0)\n",
    "\t\t\tfor bbox in img_data_aug['bboxes']:\n",
    "\t\t\t\ty1 = bbox['y1']\n",
    "\t\t\t\ty2 = bbox['y2']\n",
    "\t\t\t\tbbox['y2'] = rows - y1\n",
    "\t\t\t\tbbox['y1'] = rows - y2\n",
    "\n",
    "\t\tif config.rot_90:\n",
    "\t\t\tangle = np.random.choice([0,90,180,270],1)[0]\n",
    "\t\t\tif angle == 270:\n",
    "\t\t\t\timg = np.transpose(img, (1,0,2))\n",
    "\t\t\t\timg = cv2.flip(img, 0)\n",
    "\t\t\telif angle == 180:\n",
    "\t\t\t\timg = cv2.flip(img, -1)\n",
    "\t\t\telif angle == 90:\n",
    "\t\t\t\timg = np.transpose(img, (1,0,2))\n",
    "\t\t\t\timg = cv2.flip(img, 1)\n",
    "\t\t\telif angle == 0:\n",
    "\t\t\t\tpass\n",
    "\n",
    "\t\t\tfor bbox in img_data_aug['bboxes']:\n",
    "\t\t\t\tx1 = bbox['x1']\n",
    "\t\t\t\tx2 = bbox['x2']\n",
    "\t\t\t\ty1 = bbox['y1']\n",
    "\t\t\t\ty2 = bbox['y2']\n",
    "\t\t\t\tif angle == 270:\n",
    "\t\t\t\t\tbbox['x1'] = y1\n",
    "\t\t\t\t\tbbox['x2'] = y2\n",
    "\t\t\t\t\tbbox['y1'] = cols - x2\n",
    "\t\t\t\t\tbbox['y2'] = cols - x1\n",
    "\t\t\t\telif angle == 180:\n",
    "\t\t\t\t\tbbox['x2'] = cols - x1\n",
    "\t\t\t\t\tbbox['x1'] = cols - x2\n",
    "\t\t\t\t\tbbox['y2'] = rows - y1\n",
    "\t\t\t\t\tbbox['y1'] = rows - y2\n",
    "\t\t\t\telif angle == 90:\n",
    "\t\t\t\t\tbbox['x1'] = rows - y2\n",
    "\t\t\t\t\tbbox['x2'] = rows - y1\n",
    "\t\t\t\t\tbbox['y1'] = x1\n",
    "\t\t\t\t\tbbox['y2'] = x2        \n",
    "\t\t\t\telif angle == 0:\n",
    "\t\t\t\t\tpass\n",
    "\n",
    "\timg_data_aug['width'] = img.shape[1]\n",
    "\timg_data_aug['height'] = img.shape[0]\n",
    "\treturn img_data_aug, img\n",
    "\n",
    "# Yield the ground-truth anchors as Y (labels)\n",
    "def get_anchor_gt(all_img_data, config, mode='train'):\n",
    "    \n",
    "    while True:\n",
    "        for img_data in all_img_data:\n",
    "            try:\n",
    "                # read in image, and optionally add augmentation\n",
    "                if mode == 'train':\n",
    "                    img_data_aug, x_img = augment(img_data, config, augment=True)\n",
    "                else:\n",
    "                    img_data_aug, x_img = augment(img_data, config, augment=False)\n",
    "\n",
    "                (width, height) = (img_data_aug['width'], img_data_aug['height'])\n",
    "                (rows, cols, _) = x_img.shape\n",
    "\n",
    "                assert cols == width\n",
    "                assert rows == height\n",
    "\n",
    "                # get image dimensions for resizing\n",
    "                (resized_width, resized_height) = get_new_img_size(width, height, config.im_size)\n",
    "\n",
    "\t\t        # resize the image so that smalles side is length = 300px\n",
    "                x_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)\n",
    "                debug_img = x_img.copy()\n",
    "\n",
    "                try:\n",
    "                    y_rpn_cls, y_rpn_regr, num_pos = calc_rpn(config, img_data_aug, width, height, resized_width, resized_height)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    continue\n",
    "\n",
    "                # Zero-center by mean pixel, and preprocess image\n",
    "                x_img = x_img[:,:, (2, 1, 0)]  # BGR -> RGB\n",
    "                x_img = x_img.astype(np.float32)\n",
    "                x_img[:, :, 0] -= config.img_channel_mean[0]\n",
    "                x_img[:, :, 1] -= config.img_channel_mean[1]\n",
    "                x_img[:, :, 2] -= config.img_channel_mean[2]\n",
    "                x_img /= config.img_scaling_factor\n",
    "                x_img = np.transpose(x_img, (2, 0, 1))\n",
    "                x_img = np.expand_dims(x_img, axis=0)\n",
    "                y_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= config.std_scaling\n",
    "                x_img = np.transpose(x_img, (0, 2, 3, 1))\n",
    "                y_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))\n",
    "                y_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))\n",
    "\n",
    "                yield np.copy(x_img), [np.copy(y_rpn_cls), np.copy(y_rpn_regr)], img_data_aug, debug_img, num_pos\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "\n",
    "# Calculate union\n",
    "def union(au, bu, area_intersection):\n",
    "\tarea_a = (au[2] - au[0]) * (au[3] - au[1])\n",
    "\tarea_b = (bu[2] - bu[0]) * (bu[3] - bu[1])\n",
    "\tarea_union = area_a + area_b - area_intersection\n",
    "\treturn area_union\n",
    "\n",
    "# Calculate intersection\n",
    "def intersection(ai, bi):\n",
    "\tx = max(ai[0], bi[0])\n",
    "\ty = max(ai[1], bi[1])\n",
    "\tw = min(ai[2], bi[2]) - x\n",
    "\th = min(ai[3], bi[3]) - y\n",
    "\tif w < 0 or h < 0:\n",
    "\t\treturn 0\n",
    "\treturn w*h\n",
    "\n",
    "# Calculate IOU\n",
    "def iou(a, b):\n",
    "\t# a and b should be (x1,y1,x2,y2)\n",
    "\n",
    "\tif a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n",
    "\t\treturn 0.0\n",
    "\n",
    "\tarea_i = intersection(a, b)\n",
    "\tarea_u = union(a, b, area_i)\n",
    "\n",
    "\treturn float(area_i) / float(area_u + 1e-6)\n",
    "\n",
    "# Calculate the rpn for all anchors\n",
    "def calc_rpn(config, img_data, width, height, resized_width, resized_height):\n",
    "\t\n",
    "\tdownscale = float(config.rpn_stride) \n",
    "\tanchor_sizes = config.anchor_box_scales\n",
    "\tanchor_ratios = config.anchor_box_ratios\n",
    "\tnum_anchors = len(anchor_sizes) * len(anchor_ratios)\n",
    "\n",
    "\t# calculate the output map size based on the network architecture\n",
    "\t(output_width, output_height) = get_img_output_length(resized_width, resized_height)\n",
    "\n",
    "\tn_anchratios = len(anchor_ratios)\n",
    "\t\n",
    "\t# initialise empty output objectives\n",
    "\ty_rpn_overlap = np.zeros((output_height, output_width, num_anchors))\n",
    "\ty_is_box_valid = np.zeros((output_height, output_width, num_anchors))\n",
    "\ty_rpn_regr = np.zeros((output_height, output_width, num_anchors * 4))\n",
    "\tnum_bboxes = len(img_data['bboxes'])\n",
    "\n",
    "\tnum_anchors_for_bbox = np.zeros(num_bboxes).astype(int)\n",
    "\tbest_anchor_for_bbox = -1*np.ones((num_bboxes, 4)).astype(int)\n",
    "\tbest_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)\n",
    "\tbest_x_for_bbox = np.zeros((num_bboxes, 4)).astype(int)\n",
    "\tbest_dx_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)\n",
    "\n",
    "\t# get the GT box coordinates, and resize to account for image resizing\n",
    "\tgta = np.zeros((num_bboxes, 4))\n",
    "\tfor bbox_num, bbox in enumerate(img_data['bboxes']):\n",
    "\t\t# get the GT box coordinates, and resize to account for image resizing\n",
    "\t\tgta[bbox_num, 0] = bbox['x1'] * (resized_width / float(width))\n",
    "\t\tgta[bbox_num, 1] = bbox['x2'] * (resized_width / float(width))\n",
    "\t\tgta[bbox_num, 2] = bbox['y1'] * (resized_height / float(height))\n",
    "\t\tgta[bbox_num, 3] = bbox['y2'] * (resized_height / float(height))\n",
    "\t\n",
    "\t# rpn ground truth\n",
    "\tfor anchor_size_idx in range(len(anchor_sizes)):\n",
    "\t\tfor anchor_ratio_idx in range(n_anchratios):\n",
    "\t\t\tanchor_x = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][0]\n",
    "\t\t\tanchor_y = anchor_sizes[anchor_size_idx] * anchor_ratios[anchor_ratio_idx][1]\t\n",
    "\t\t\t\n",
    "\t\t\tfor ix in range(output_width):\t\t\t\t\t\n",
    "\t\t\t\t# x-coordinates of the current anchor box\t\n",
    "\t\t\t\tx1_anc = downscale * (ix + 0.5) - anchor_x / 2\n",
    "\t\t\t\tx2_anc = downscale * (ix + 0.5) + anchor_x / 2\t\n",
    "\t\t\t\t\n",
    "\t\t\t\t# ignore boxes that go across image boundaries\t\t\t\t\t\n",
    "\t\t\t\tif x1_anc < 0 or x2_anc > resized_width:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\n",
    "\t\t\t\tfor jy in range(output_height):\n",
    "\n",
    "\t\t\t\t\t# y-coordinates of the current anchor box\n",
    "\t\t\t\t\ty1_anc = downscale * (jy + 0.5) - anchor_y / 2\n",
    "\t\t\t\t\ty2_anc = downscale * (jy + 0.5) + anchor_y / 2\n",
    "\n",
    "\t\t\t\t\t# ignore boxes that go across image boundaries\n",
    "\t\t\t\t\tif y1_anc < 0 or y2_anc > resized_height:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t\t# bbox_type indicates whether an anchor should be a target\n",
    "\t\t\t\t\t# Initialize with 'negative'\n",
    "\t\t\t\t\tbbox_type = 'neg'\n",
    "\n",
    "\t\t\t\t\t# this is the best IOU for the (x,y) coord and the current anchor\n",
    "\t\t\t\t\t# note that this is different from the best IOU for a GT bbox\n",
    "\t\t\t\t\tbest_iou_for_loc = 0.0\n",
    "\n",
    "\t\t\t\t\tfor bbox_num in range(num_bboxes):\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\t# get IOU of the current GT box and the current anchor box\n",
    "\t\t\t\t\t\tcurr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1_anc, y1_anc, x2_anc, y2_anc])\n",
    "\t\t\t\t\t\t# calculate the regression targets if they will be needed\n",
    "\t\t\t\t\t\tif curr_iou > best_iou_for_bbox[bbox_num] or curr_iou > config.rpn_max_overlap:\n",
    "\t\t\t\t\t\t\tcx = (gta[bbox_num, 0] + gta[bbox_num, 1]) / 2.0\n",
    "\t\t\t\t\t\t\tcy = (gta[bbox_num, 2] + gta[bbox_num, 3]) / 2.0\n",
    "\t\t\t\t\t\t\tcxa = (x1_anc + x2_anc)/2.0\n",
    "\t\t\t\t\t\t\tcya = (y1_anc + y2_anc)/2.0\n",
    "\n",
    "\t\t\t\t\t\t\t# x,y are the center point of ground-truth bbox\n",
    "\t\t\t\t\t\t\t# xa,ya are the center point of anchor bbox (xa=downscale * (ix + 0.5); ya=downscale * (iy+0.5))\n",
    "\t\t\t\t\t\t\t# w,h are the width and height of ground-truth bbox\n",
    "\t\t\t\t\t\t\t# wa,ha are the width and height of anchor bboxe\n",
    "\t\t\t\t\t\t\t# tx = (x - xa) / wa\n",
    "\t\t\t\t\t\t\t# ty = (y - ya) / ha\n",
    "\t\t\t\t\t\t\t# tw = log(w / wa)\n",
    "\t\t\t\t\t\t\t# th = log(h / ha)\n",
    "\t\t\t\t\t\t\ttx = (cx - cxa) / (x2_anc - x1_anc)\n",
    "\t\t\t\t\t\t\tty = (cy - cya) / (y2_anc - y1_anc)\n",
    "\t\t\t\t\t\t\ttw = np.log((gta[bbox_num, 1] - gta[bbox_num, 0]) / (x2_anc - x1_anc))\n",
    "\t\t\t\t\t\t\tth = np.log((gta[bbox_num, 3] - gta[bbox_num, 2]) / (y2_anc - y1_anc))\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\t\t\tif img_data['bboxes'][bbox_num]['class'] != 'bg':\n",
    "\n",
    "\t\t\t\t\t\t\t# all GT boxes should be mapped to an anchor box, so we keep track of which anchor box was best\n",
    "\t\t\t\t\t\t\tif curr_iou > best_iou_for_bbox[bbox_num]:\n",
    "\t\t\t\t\t\t\t\tbest_anchor_for_bbox[bbox_num] = [jy, ix, anchor_ratio_idx, anchor_size_idx]\n",
    "\t\t\t\t\t\t\t\tbest_iou_for_bbox[bbox_num] = curr_iou\n",
    "\t\t\t\t\t\t\t\tbest_x_for_bbox[bbox_num,:] = [x1_anc, x2_anc, y1_anc, y2_anc]\n",
    "\t\t\t\t\t\t\t\tbest_dx_for_bbox[bbox_num,:] = [tx, ty, tw, th]\n",
    "\n",
    "\t\t\t\t\t\t\t# we set the anchor to positive if the IOU is >0.7 (it does not matter if there was another better box, it just indicates overlap)\n",
    "\t\t\t\t\t\t\tif curr_iou > config.rpn_max_overlap:\n",
    "\t\t\t\t\t\t\t\tbbox_type = 'pos'\n",
    "\t\t\t\t\t\t\t\tnum_anchors_for_bbox[bbox_num] += 1\n",
    "\t\t\t\t\t\t\t\t# we update the regression layer target if this IOU is the best for the current (x,y) and anchor position\n",
    "\t\t\t\t\t\t\t\tif curr_iou > best_iou_for_loc:\n",
    "\t\t\t\t\t\t\t\t\tbest_iou_for_loc = curr_iou\n",
    "\t\t\t\t\t\t\t\t\tbest_regr = (tx, ty, tw, th)\n",
    "\n",
    "\t\t\t\t\t\t\t# if the IOU is >0.3 and <0.7, it is ambiguous and no included in the objective\n",
    "\t\t\t\t\t\t\tif config.rpn_min_overlap < curr_iou < config.rpn_max_overlap:\n",
    "\t\t\t\t\t\t\t\t# gray zone between neg and pos\n",
    "\t\t\t\t\t\t\t\tif bbox_type != 'pos':\n",
    "\t\t\t\t\t\t\t\t\tbbox_type = 'neutral'\n",
    "\n",
    "\t\t\t\t\t# turn on or off outputs depending on IOUs\n",
    "\t\t\t\t\tif bbox_type == 'neg':\n",
    "\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
    "\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
    "\t\t\t\t\telif bbox_type == 'neutral':\n",
    "\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
    "\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 0\n",
    "\t\t\t\t\telif bbox_type == 'pos':\n",
    "\t\t\t\t\t\ty_is_box_valid[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
    "\t\t\t\t\t\ty_rpn_overlap[jy, ix, anchor_ratio_idx + n_anchratios * anchor_size_idx] = 1\n",
    "\t\t\t\t\t\tstart = 4 * (anchor_ratio_idx + n_anchratios * anchor_size_idx)\n",
    "\t\t\t\t\t\ty_rpn_regr[jy, ix, start:start+4] = best_regr\n",
    "\n",
    "\t# we ensure that every bbox has at least one positive RPN region\n",
    "\tfor idx in range(num_anchors_for_bbox.shape[0]):\n",
    "\t\tif num_anchors_for_bbox[idx] == 0:\n",
    "\t\t\t# no box with an IOU greater than zero ...\n",
    "\t\t\tif best_anchor_for_bbox[idx, 0] == -1:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\ty_is_box_valid[\n",
    "\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n",
    "\t\t\t\tbest_anchor_for_bbox[idx,3]] = 1\n",
    "\t\t\ty_rpn_overlap[\n",
    "\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], best_anchor_for_bbox[idx,2] + n_anchratios *\n",
    "\t\t\t\tbest_anchor_for_bbox[idx,3]] = 1\n",
    "\t\t\tstart = 4 * (best_anchor_for_bbox[idx,2] + n_anchratios * best_anchor_for_bbox[idx,3])\n",
    "\t\t\ty_rpn_regr[\n",
    "\t\t\t\tbest_anchor_for_bbox[idx,0], best_anchor_for_bbox[idx,1], start:start+4] = best_dx_for_bbox[idx, :]\n",
    "\n",
    "\ty_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n",
    "\ty_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n",
    "\n",
    "\ty_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n",
    "\ty_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n",
    "\n",
    "\ty_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n",
    "\ty_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n",
    "\n",
    "\tpos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))\n",
    "\tneg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\n",
    "\n",
    "\tnum_pos = len(pos_locs[0])\n",
    "\n",
    "\t# one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative\n",
    "\t# regions. We also limit it to 256 regions.\n",
    "\tnum_regions = 256\n",
    "\n",
    "\tif len(pos_locs[0]) > num_regions/2:\n",
    "\t\tval_locs = random.sample(range(len(pos_locs[0])), len(pos_locs[0]) - num_regions/2)\n",
    "\t\ty_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n",
    "\t\tnum_pos = num_regions/2\n",
    "\n",
    "\tif len(neg_locs[0]) + num_pos > num_regions:\n",
    "\t\tval_locs = random.sample(range(len(neg_locs[0])), len(neg_locs[0]) - num_pos)\n",
    "\t\ty_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n",
    "\n",
    "\ty_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)\n",
    "\ty_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)\n",
    "\n",
    "\treturn np.copy(y_rpn_cls), np.copy(y_rpn_regr), num_pos\n",
    "\n",
    "# Non max suppression: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n",
    "def non_max_suppression_fast(boxes, probs, overlap_thresh=0.9, max_boxes=300):\n",
    "\n",
    "    # if there are no boxes, return an empty list\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    # grab the coordinates of the bounding boxes\n",
    "    x1 = boxes[:, 0]\n",
    "    y1 = boxes[:, 1]\n",
    "    x2 = boxes[:, 2]\n",
    "    y2 = boxes[:, 3]\n",
    "\n",
    "    np.testing.assert_array_less(x1, x2)\n",
    "    np.testing.assert_array_less(y1, y2)\n",
    "\n",
    "    # if the bounding boxes integers, convert them to floats --\n",
    "    # this is important since we'll be doing a bunch of divisions\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    "\n",
    "    # initialize the list of picked indexes\t\n",
    "    pick = []\n",
    "\n",
    "    # calculate the areas\n",
    "    area = (x2 - x1) * (y2 - y1)\n",
    "\n",
    "    # sort the bounding boxes \n",
    "    idxs = np.argsort(probs)\n",
    "\n",
    "    # keep looping while some indexes still remain in the indexes\n",
    "    # list\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last index in the indexes list and add the\n",
    "        # index value to the list of picked indexes\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    "\n",
    "        # find the intersection\n",
    "\n",
    "        xx1_int = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1_int = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2_int = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2_int = np.minimum(y2[i], y2[idxs[:last]])\n",
    "\n",
    "        ww_int = np.maximum(0, xx2_int - xx1_int)\n",
    "        hh_int = np.maximum(0, yy2_int - yy1_int)\n",
    "\n",
    "        area_int = ww_int * hh_int\n",
    "\n",
    "        # find the union\n",
    "        area_union = area[i] + area[idxs[:last]] - area_int\n",
    "\n",
    "        # compute the ratio of overlap\n",
    "        overlap = area_int/(area_union + 1e-6)\n",
    "\n",
    "        # delete all indexes from the index list that have\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlap_thresh)[0])))\n",
    "\n",
    "        if len(pick) >= max_boxes:\n",
    "            break\n",
    "\n",
    "    # return only the bounding boxes that were picked using the integer data type\n",
    "    boxes = boxes[pick].astype(\"int\")\n",
    "    probs = probs[pick]\n",
    "    return boxes, probs\n",
    "\n",
    "# Converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
    "def calc_iou(R, img_data, config, class_mapping):\n",
    "\n",
    "    bboxes = img_data['bboxes']\n",
    "    (width, height) = (img_data['width'], img_data['height'])\n",
    "    # get image dimensions for resizing\n",
    "    (resized_width, resized_height) = get_new_img_size(width, height, config.im_size)\n",
    "\n",
    "    gta = np.zeros((len(bboxes), 4))\n",
    "\n",
    "    for bbox_num, bbox in enumerate(bboxes):\n",
    "        # get the GT box coordinates, and resize to account for image resizing\n",
    "        # gta[bbox_num, 0] = (40 * (600 / 800)) / 16 = int(round(1.875)) = 2 (x in feature map)\n",
    "        gta[bbox_num, 0] = int(round(bbox['x1'] * (resized_width / float(width))/config.rpn_stride))\n",
    "        gta[bbox_num, 1] = int(round(bbox['x2'] * (resized_width / float(width))/config.rpn_stride))\n",
    "        gta[bbox_num, 2] = int(round(bbox['y1'] * (resized_height / float(height))/config.rpn_stride))\n",
    "        gta[bbox_num, 3] = int(round(bbox['y2'] * (resized_height / float(height))/config.rpn_stride))\n",
    "\n",
    "    x_roi = []\n",
    "    y_class_num = []\n",
    "    y_class_regr_coords = []\n",
    "    y_class_regr_label = []\n",
    "    IoUs = [] # for debugging only\n",
    "\n",
    "    # R.shape[0]: number of bboxes (=300 from non_max_suppression)\n",
    "    for ix in range(R.shape[0]):\n",
    "        (x1, y1, x2, y2) = R[ix, :]\n",
    "        x1 = int(round(x1))\n",
    "        y1 = int(round(y1))\n",
    "        x2 = int(round(x2))\n",
    "        y2 = int(round(y2))\n",
    "\n",
    "        best_iou = 0.0\n",
    "        best_bbox = -1\n",
    "        # Iterate through all the ground-truth bboxes to calculate the iou\n",
    "        for bbox_num in range(len(bboxes)):\n",
    "            curr_iou = iou([gta[bbox_num, 0], gta[bbox_num, 2], gta[bbox_num, 1], gta[bbox_num, 3]], [x1, y1, x2, y2])\n",
    "\n",
    "            # Find out the corresponding ground-truth bbox_num with larget iou\n",
    "            if curr_iou > best_iou:\n",
    "                best_iou = curr_iou\n",
    "                best_bbox = bbox_num\n",
    "\n",
    "        if best_iou < config.classifier_min_overlap:\n",
    "                continue\n",
    "        else:\n",
    "            w = x2 - x1\n",
    "            h = y2 - y1\n",
    "            x_roi.append([x1, y1, w, h])\n",
    "            IoUs.append(best_iou)\n",
    "\n",
    "            if config.classifier_min_overlap <= best_iou < config.classifier_max_overlap:\n",
    "                # hard negative example\n",
    "                cls_name = 'bg'\n",
    "            elif config.classifier_max_overlap <= best_iou:\n",
    "                cls_name = bboxes[best_bbox]['class']\n",
    "                cxg = (gta[best_bbox, 0] + gta[best_bbox, 1]) / 2.0\n",
    "                cyg = (gta[best_bbox, 2] + gta[best_bbox, 3]) / 2.0\n",
    "\n",
    "                cx = x1 + w / 2.0\n",
    "                cy = y1 + h / 2.0\n",
    "\n",
    "                tx = (cxg - cx) / float(w)\n",
    "                ty = (cyg - cy) / float(h)\n",
    "                tw = np.log((gta[best_bbox, 1] - gta[best_bbox, 0]) / float(w))\n",
    "                th = np.log((gta[best_bbox, 3] - gta[best_bbox, 2]) / float(h))\n",
    "            else:\n",
    "                print('roi = {}'.format(best_iou))\n",
    "                raise RuntimeError\n",
    "\n",
    "        class_num = class_mapping[cls_name]\n",
    "        class_label = len(class_mapping) * [0]\n",
    "        class_label[class_num] = 1\n",
    "        y_class_num.append(copy.deepcopy(class_label))\n",
    "        coords = [0] * 4 * (len(class_mapping) - 1)\n",
    "        labels = [0] * 4 * (len(class_mapping) - 1)\n",
    "        if cls_name != 'bg':\n",
    "            label_pos = 4 * class_num\n",
    "            sx, sy, sw, sh = config.classifier_regr_std\n",
    "            coords[label_pos:4+label_pos] = [sx*tx, sy*ty, sw*tw, sh*th]\n",
    "            labels[label_pos:4+label_pos] = [1, 1, 1, 1]\n",
    "            y_class_regr_coords.append(copy.deepcopy(coords))\n",
    "            y_class_regr_label.append(copy.deepcopy(labels))\n",
    "        else:\n",
    "            y_class_regr_coords.append(copy.deepcopy(coords))\n",
    "            y_class_regr_label.append(copy.deepcopy(labels))\n",
    "\n",
    "    if len(x_roi) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # bboxes that iou > config.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes\n",
    "    X = np.array(x_roi)\n",
    "    # one hot code for bboxes from above => x_roi (X)\n",
    "    Y1 = np.array(y_class_num)\n",
    "    # corresponding labels and corresponding gt bboxes\n",
    "    Y2 = np.concatenate([np.array(y_class_regr_label),np.array(y_class_regr_coords)],axis=1)\n",
    "\n",
    "    return np.expand_dims(X, axis=0), np.expand_dims(Y1, axis=0), np.expand_dims(Y2, axis=0), IoUs\n",
    "\n",
    "# Formats the image size based on config\n",
    "def format_img_size(img, config):\n",
    "\n",
    "\timg_min_side = float(config.im_size)\n",
    "\t(height,width,_) = img.shape\n",
    "\t\t\n",
    "\tif width <= height:\n",
    "\t\tratio = img_min_side/width\n",
    "\t\tnew_height = int(ratio * height)\n",
    "\t\tnew_width = int(img_min_side)\n",
    "\telse:\n",
    "\t\tratio = img_min_side/height\n",
    "\t\tnew_width = int(ratio * width)\n",
    "\t\tnew_height = int(img_min_side)\n",
    "\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "\treturn img, ratio\t\n",
    "\n",
    "# Formats the image channels based on config\n",
    "def format_img_channels(img, config):\n",
    "\timg = img[:, :, (2, 1, 0)]\n",
    "\timg = img.astype(np.float32)\n",
    "\timg[:, :, 0] -= config.img_channel_mean[0]\n",
    "\timg[:, :, 1] -= config.img_channel_mean[1]\n",
    "\timg[:, :, 2] -= config.img_channel_mean[2]\n",
    "\timg /= config.img_scaling_factor\n",
    "\timg = np.transpose(img, (2, 0, 1))\n",
    "\timg = np.expand_dims(img, axis=0)\n",
    "\treturn img\n",
    "\n",
    "# Formats an image for model prediction based on config\n",
    "def format_img(img, config):\n",
    "\timg, ratio = format_img_size(img, config)\n",
    "\timg = format_img_channels(img, config)\n",
    "\treturn img, ratio\n",
    "\n",
    "# Method to transform the coordinates of the bounding box to its original size\n",
    "def get_real_coordinates(ratio, x1, y1, x2, y2):\n",
    "\n",
    "\treal_x1 = int(round(x1 // ratio))\n",
    "\treal_y1 = int(round(y1 // ratio))\n",
    "\treal_x2 = int(round(x2 // ratio))\n",
    "\treal_y2 = int(round(y2 // ratio))\n",
    "\n",
    "\treturn (real_x1, real_y1, real_x2 ,real_y2)\n",
    "\n",
    "# Get map\n",
    "def get_map(pred, gt, f):\n",
    "\tT = {}\n",
    "\tP = {}\n",
    "\tfx, fy = f\n",
    "\n",
    "\tfor bbox in gt:\n",
    "\t\tbbox['bbox_matched'] = False\n",
    "\n",
    "\tpred_probs = np.array([s['prob'] for s in pred])\n",
    "\tbox_idx_sorted_by_prob = np.argsort(pred_probs)[::-1]\n",
    "\n",
    "\tfor box_idx in box_idx_sorted_by_prob:\n",
    "\t\tpred_box = pred[box_idx]\n",
    "\t\tpred_class = pred_box['class']\n",
    "\t\tpred_x1 = pred_box['x1']\n",
    "\t\tpred_x2 = pred_box['x2']\n",
    "\t\tpred_y1 = pred_box['y1']\n",
    "\t\tpred_y2 = pred_box['y2']\n",
    "\t\tpred_prob = pred_box['prob']\n",
    "\t\tif pred_class not in P:\n",
    "\t\t\tP[pred_class] = []\n",
    "\t\t\tT[pred_class] = []\n",
    "\t\tP[pred_class].append(pred_prob)\n",
    "\t\tfound_match = False\n",
    "\n",
    "\t\tfor gt_box in gt:\n",
    "\t\t\tgt_class = gt_box['class']\n",
    "\t\t\tgt_x1 = gt_box['x1']/fx\n",
    "\t\t\tgt_x2 = gt_box['x2']/fx\n",
    "\t\t\tgt_y1 = gt_box['y1']/fy\n",
    "\t\t\tgt_y2 = gt_box['y2']/fy\n",
    "\t\t\tgt_seen = gt_box['bbox_matched']\n",
    "\t\t\tif gt_class != pred_class:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tif gt_seen:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tiou_map = iou((pred_x1, pred_y1, pred_x2, pred_y2), (gt_x1, gt_y1, gt_x2, gt_y2))\n",
    "\t\t\tif iou_map >= 0.5:\n",
    "\t\t\t\tfound_match = True\n",
    "\t\t\t\tgt_box['bbox_matched'] = True\n",
    "\t\t\t\tbreak\n",
    "\t\t\telse:\n",
    "\t\t\t\tcontinue\n",
    "\n",
    "\t\tT[pred_class].append(int(found_match))\n",
    "\n",
    "\tfor gt_box in gt:\n",
    "\t\tif not gt_box['bbox_matched']:# and not gt_box['difficult']:\n",
    "\t\t\tif gt_box['class'] not in P:\n",
    "\t\t\t\tP[gt_box['class']] = []\n",
    "\t\t\t\tT[gt_box['class']] = []\n",
    "\n",
    "\t\t\tT[gt_box['class']].append(1)\n",
    "\t\t\tP[gt_box['class']].append(0)\n",
    "\n",
    "\t#import pdb\n",
    "\t#pdb.set_trace()\n",
    "\treturn T, P\n",
    "\n",
    "# Format image for map. Resize original image to config.im_size (300 in here)\n",
    "def format_img_map(img, config):\n",
    "\n",
    "\timg_min_side = float(config.im_size)\n",
    "\t(height,width,_) = img.shape\n",
    "\t\n",
    "\tif width <= height:\n",
    "\t\tf = img_min_side/width\n",
    "\t\tnew_height = int(f * height)\n",
    "\t\tnew_width = int(img_min_side)\n",
    "\telse:\n",
    "\t\tf = img_min_side/height\n",
    "\t\tnew_width = int(f * width)\n",
    "\t\tnew_height = int(img_min_side)\n",
    "\tfx = width/float(new_width)\n",
    "\tfy = height/float(new_height)\n",
    "\timg = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_CUBIC)\n",
    "\t# Change image channel from BGR to RGB\n",
    "\timg = img[:, :, (2, 1, 0)]\n",
    "\timg = img.astype(np.float32)\n",
    "\timg[:, :, 0] -= config.img_channel_mean[0]\n",
    "\timg[:, :, 1] -= config.img_channel_mean[1]\n",
    "\timg[:, :, 2] -= config.img_channel_mean[2]\n",
    "\timg /= config.img_scaling_factor\n",
    "\t# Change img shape from (height, width, channel) to (channel, height, width)\n",
    "\timg = np.transpose(img, (2, 0, 1))\n",
    "\t# Expand one dimension at axis 0\n",
    "\t# img shape becames (1, channel, height, width)\n",
    "\timg = np.expand_dims(img, axis=0)\n",
    "\treturn img, fx, fy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n",
    "\n",
    "This module includes custom layers that is used by the model builder to create a Faster R-CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import cv2\n",
    "import keras\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import Chaka_Faster_rcnn.common as common\n",
    "\n",
    "\n",
    "# ROI pooling layer for 2D inputs\n",
    "# K. He, X. Zhang, S. Ren, J. Sun\n",
    "class RoiPoolingConv(keras.engine.Layer):\n",
    "\n",
    "    def __init__(self, pool_size, num_rois, **kwargs):\n",
    "\n",
    "        # keras.backend.image_dim_ordering()\n",
    "        self.dim_ordering = keras.backend.image_data_format()\n",
    "        self.pool_size = pool_size\n",
    "        self.num_rois = num_rois\n",
    "\n",
    "        super(RoiPoolingConv, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.nb_channels = input_shape[0][3]   \n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        assert(len(x) == 2)\n",
    "\n",
    "        # x[0] is image with shape (rows, cols, channels)\n",
    "        img = x[0]\n",
    "\n",
    "        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n",
    "        rois = x[1]\n",
    "\n",
    "        input_shape = keras.backend.shape(img)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for roi_idx in range(self.num_rois):\n",
    "\n",
    "            x = rois[0, roi_idx, 0]\n",
    "            y = rois[0, roi_idx, 1]\n",
    "            w = rois[0, roi_idx, 2]\n",
    "            h = rois[0, roi_idx, 3]\n",
    "\n",
    "            x = keras.backend.cast(x, 'int32')\n",
    "            y = keras.backend.cast(y, 'int32')\n",
    "            w = keras.backend.cast(w, 'int32')\n",
    "            h = keras.backend.cast(h, 'int32')\n",
    "\n",
    "            # Resized roi of the image to pooling size (7x7)\n",
    "            # tf.image.resize_images\n",
    "            rs = tf.image.resize(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n",
    "            outputs.append(rs)\n",
    "                \n",
    "        # Concentate tensors along an axis\n",
    "        final_output = keras.backend.concatenate(outputs, axis=0)\n",
    "\n",
    "        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n",
    "        # Might be (1, 4, 7, 7, 3)\n",
    "        final_output = keras.backend.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n",
    "\n",
    "        # permute_dimensions is similar to transpose\n",
    "        final_output = keras.backend.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
    "\n",
    "        # Return final output\n",
    "        return final_output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'pool_size': self.pool_size, 'num_rois': self.num_rois}\n",
    "        base_config = super(RoiPoolingConv, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "# Create a rpn layer\n",
    "def rpn_layer(base_layers, num_anchors):\n",
    "\n",
    "    x = keras.layers.Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n",
    "    x_class = keras.layers.Conv2D(num_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_out_class')(x)\n",
    "    x_regr = keras.layers.Conv2D(num_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_out_regress')(x)\n",
    "\n",
    "    return [x_class, x_regr, base_layers]\n",
    "\n",
    "# Create a classifier layer\n",
    "def classifier_layer(base_layers, input_rois, num_rois, nb_classes = 2):\n",
    "\n",
    "    input_shape = (num_rois,7,7,512)\n",
    "    pooling_regions = 7\n",
    "\n",
    "    # out_roi_pool.shape = (1, num_rois, channels, pool_size, pool_size)\n",
    "    # num_rois (4) 7x7 roi pooling\n",
    "    out_roi_pool = RoiPoolingConv(pooling_regions, num_rois)([base_layers, input_rois])\n",
    "\n",
    "    # Flatten the convlutional layer and connected to 2 FC and 2 dropout\n",
    "    out = keras.layers.TimeDistributed(keras.layers.Flatten(name='flatten'))(out_roi_pool)\n",
    "    out = keras.layers.TimeDistributed(keras.layers.Dense(4096, activation='relu', name='fc1'))(out)\n",
    "    out = keras.layers.TimeDistributed(keras.layers.Dropout(0.5))(out)\n",
    "    out = keras.layers.TimeDistributed(keras.layers.Dense(4096, activation='relu', name='fc2'))(out)\n",
    "    out = keras.layers.TimeDistributed(keras.layers.Dropout(0.5))(out)\n",
    "\n",
    "    # There are two output layer\n",
    "    # out_class: softmax acivation function for classify the class name of the object\n",
    "    # out_regr: linear activation function for bboxes coordinates regression\n",
    "    out_class = keras.layers.TimeDistributed(keras.layers.Dense(nb_classes, activation='softmax', kernel_initializer='zero'), name='dense_class_{}'.format(nb_classes))(out)\n",
    "    # note: no regression target for bg class\n",
    "    out_regr = keras.layers.TimeDistributed(keras.layers.Dense(4 * (nb_classes-1), activation='linear', kernel_initializer='zero'), name='dense_regress_{}'.format(nb_classes))(out)\n",
    "\n",
    "    return [out_class, out_regr]\n",
    "\n",
    "# Apply regression layer to all anchors in one feature map\n",
    "def apply_regr_np(X, T):\n",
    "\n",
    "    try:\n",
    "        x = X[0, :, :]\n",
    "        y = X[1, :, :]\n",
    "        w = X[2, :, :]\n",
    "        h = X[3, :, :]\n",
    "\n",
    "        tx = T[0, :, :]\n",
    "        ty = T[1, :, :]\n",
    "        tw = T[2, :, :]\n",
    "        th = T[3, :, :]\n",
    "\n",
    "        cx = x + w/2.\n",
    "        cy = y + h/2.\n",
    "        cx1 = tx * w + cx\n",
    "        cy1 = ty * h + cy\n",
    "\n",
    "        w1 = np.exp(tw.astype(np.float64)) * w\n",
    "        h1 = np.exp(th.astype(np.float64)) * h\n",
    "        x1 = cx1 - w1/2.\n",
    "        y1 = cy1 - h1/2.\n",
    "\n",
    "        x1 = np.round(x1)\n",
    "        y1 = np.round(y1)\n",
    "        w1 = np.round(w1)\n",
    "        h1 = np.round(h1)\n",
    "        return np.stack([x1, y1, w1, h1])\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return X\n",
    "   \n",
    "# Apply regression to x, y, w and h\n",
    "def apply_regr(x, y, w, h, tx, ty, tw, th):\n",
    "\n",
    "    try:\n",
    "        cx = x + w/2.\n",
    "        cy = y + h/2.\n",
    "        cx1 = tx * w + cx\n",
    "        cy1 = ty * h + cy\n",
    "        w1 = math.exp(tw) * w\n",
    "        h1 = math.exp(th) * h\n",
    "        x1 = cx1 - w1/2.\n",
    "        y1 = cy1 - h1/2.\n",
    "        x1 = int(round(x1))\n",
    "        y1 = int(round(y1))\n",
    "        w1 = int(round(w1))\n",
    "        h1 = int(round(h1))\n",
    "\n",
    "        return x1, y1, w1, h1\n",
    "\n",
    "    except ValueError:\n",
    "        return x, y, w, h\n",
    "    except OverflowError:\n",
    "        return x, y, w, h\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return x, y, w, h\n",
    "\n",
    "# Convert rpn layer to roi bboxes\n",
    "def rpn_to_roi(rpn_layer, regr_layer, config, dim_ordering, use_regr=True, max_boxes=300,overlap_thresh=0.9):\n",
    "\t\n",
    "    # Make sure that there is configurations\n",
    "    if config == None:\n",
    "        config = common.Config()\n",
    "\n",
    "    # Create regression layer\n",
    "    regr_layer = regr_layer / config.std_scaling\n",
    "\n",
    "    anchor_sizes = config.anchor_box_scales   # (3 in here)\n",
    "    anchor_ratios = config.anchor_box_ratios  # (3 in here)\n",
    "\n",
    "    assert rpn_layer.shape[0] == 1\n",
    "\n",
    "    (rows, cols) = rpn_layer.shape[1:3]\n",
    "\n",
    "    curr_layer = 0\n",
    "\n",
    "    # A.shape = (4, feature_map.height, feature_map.width, num_anchors) \n",
    "    # Might be (4, 18, 25, 18) if resized image is 400 width and 300\n",
    "    # A is the coordinates for 9 anchors for every point in the feature map \n",
    "    # => all 18x25x9=4050 anchors cooridnates\n",
    "    A = np.zeros((4, rpn_layer.shape[1], rpn_layer.shape[2], rpn_layer.shape[3]))\n",
    "\n",
    "    for anchor_size in anchor_sizes:\n",
    "        for anchor_ratio in anchor_ratios:\n",
    "            # anchor_x = (128 * 1) / 16 = 8  => width of current anchor\n",
    "            # anchor_y = (128 * 2) / 16 = 16 => height of current anchor\n",
    "            anchor_x = (anchor_size * anchor_ratio[0])/config.rpn_stride\n",
    "            anchor_y = (anchor_size * anchor_ratio[1])/config.rpn_stride\n",
    "\n",
    "            # curr_layer: 0~8 (9 anchors)\n",
    "            # the Kth anchor of all position in the feature map (9th in total)\n",
    "            regr = regr_layer[0, :, :, 4 * curr_layer:4 * curr_layer + 4] # shape => (18, 25, 4)\n",
    "            regr = np.transpose(regr, (2, 0, 1)) # shape => (4, 18, 25)\n",
    "\n",
    "            # Create 18x25 mesh grid\n",
    "            # For every point in x, there are all the y points and vice versa\n",
    "            # X.shape = (18, 25)\n",
    "            # Y.shape = (18, 25)\n",
    "            X, Y = np.meshgrid(np.arange(cols),np. arange(rows))\n",
    "\n",
    "            # Calculate anchor position and size for each feature map point\n",
    "            A[0, :, :, curr_layer] = X - anchor_x/2 # Top left x coordinate\n",
    "            A[1, :, :, curr_layer] = Y - anchor_y/2 # Top left y coordinate\n",
    "            A[2, :, :, curr_layer] = anchor_x       # width of current anchor\n",
    "            A[3, :, :, curr_layer] = anchor_y       # height of current anchor\n",
    "\n",
    "            # Apply regression to x, y, w and h if there is rpn regression layer\n",
    "            if use_regr:\n",
    "                A[:, :, :, curr_layer] = apply_regr_np(A[:, :, :, curr_layer], regr)\n",
    "\n",
    "            # Avoid width and height exceeding 1\n",
    "            A[2, :, :, curr_layer] = np.maximum(1, A[2, :, :, curr_layer])\n",
    "            A[3, :, :, curr_layer] = np.maximum(1, A[3, :, :, curr_layer])\n",
    "\n",
    "            # Convert (x, y , w, h) to (x1, y1, x2, y2)\n",
    "            # x1, y1 is top left coordinate\n",
    "            # x2, y2 is bottom right coordinate\n",
    "            A[2, :, :, curr_layer] += A[0, :, :, curr_layer]\n",
    "            A[3, :, :, curr_layer] += A[1, :, :, curr_layer]\n",
    "\n",
    "            # Avoid bboxes drawn outside the feature map\n",
    "            A[0, :, :, curr_layer] = np.maximum(0, A[0, :, :, curr_layer])\n",
    "            A[1, :, :, curr_layer] = np.maximum(0, A[1, :, :, curr_layer])\n",
    "            A[2, :, :, curr_layer] = np.minimum(cols-1, A[2, :, :, curr_layer])\n",
    "            A[3, :, :, curr_layer] = np.minimum(rows-1, A[3, :, :, curr_layer])\n",
    "\n",
    "            curr_layer += 1\n",
    "\n",
    "    all_boxes = np.reshape(A.transpose((0, 3, 1, 2)), (4, -1)).transpose((1, 0))  # shape=(4050, 4)\n",
    "    all_probs = rpn_layer.transpose((0, 3, 1, 2)).reshape((-1))                   # shape=(4050,)\n",
    "\n",
    "    x1 = all_boxes[:, 0]\n",
    "    y1 = all_boxes[:, 1]\n",
    "    x2 = all_boxes[:, 2]\n",
    "    y2 = all_boxes[:, 3]\n",
    "\n",
    "    # Find out the bboxes which is illegal and delete them from bboxes list\n",
    "    idxs = np.where((x1 - x2 >= 0) | (y1 - y2 >= 0))\n",
    "\n",
    "    all_boxes = np.delete(all_boxes, idxs, 0)\n",
    "    all_probs = np.delete(all_probs, idxs, 0)\n",
    "\n",
    "    # Apply non_max_suppression\n",
    "    # Only extract the bboxes. Don't need rpn probs in the later process\n",
    "    result = common.non_max_suppression_fast(all_boxes, all_probs, overlap_thresh=overlap_thresh, max_boxes=max_boxes)[0]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Builder\n",
    "\n",
    "This module is used to create training models and inference models. A VGG-16 model is used as a backbone model for image classification, it was choosed to speed up training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import keras\n",
    "import Chaka_Faster_rcnn.common as common\n",
    "import  Chaka_Faster_rcnn.layers as layers\n",
    "\n",
    "# Get a VGG-16 model\n",
    "def get_vgg_16_model(input=None):\n",
    "\n",
    "    # Make sure that the input is okay\n",
    "    input_shape = (None, None, 3)\n",
    "    if input is None:\n",
    "        input = keras.layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if not keras.backend.is_keras_tensor(input):\n",
    "            input = keras.layers.Input(tensor=input, shape=input_shape)\n",
    "\n",
    "    # Set backbone axis\n",
    "    bn_axis = 3\n",
    "\n",
    "    # Block 1\n",
    "    x = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(input)\n",
    "    x = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "    # Block 2\n",
    "    x = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "    x = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "    # Block 3\n",
    "    x = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\n",
    "    x = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\n",
    "    x = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
    "\n",
    "    # Block 4\n",
    "    x = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\n",
    "    x = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\n",
    "    x = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\n",
    "    x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
    "\n",
    "    # Block 5\n",
    "    x = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\n",
    "    x = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\n",
    "    x = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\n",
    "    # x = keras.layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "# Get training models\n",
    "def get_training_models(config=None, num_classes=2, weights_path=None):\n",
    "\n",
    "    # Make sure that there is configurations\n",
    "    if config == None:\n",
    "        config = common.Config()\n",
    "\n",
    "    # Calculate the number of anchors\n",
    "    num_anchors = len(config.anchor_box_scales) * len(config.anchor_box_ratios)\n",
    "\n",
    "    # Create input layers\n",
    "    img_input = keras.layers.Input(shape=(None, None, 3))\n",
    "    roi_input = keras.layers.Input(shape=(None, 4))\n",
    "\n",
    "    # Get a backbone model (VGG here, can be Resnet50, Inception, etc)\n",
    "    backbone = get_vgg_16_model(img_input)\n",
    "\n",
    "    # Create an rpn layer\n",
    "    rpn = layers.rpn_layer(backbone, num_anchors)\n",
    "\n",
    "    # Create a classifier\n",
    "    classifier = layers.classifier_layer(backbone, roi_input, config.num_rois, nb_classes=num_classes)\n",
    "\n",
    "    # Create models\n",
    "    rpn_model = keras.models.Model(img_input, rpn[:2])\n",
    "    classifier_model = keras.models.Model([img_input, roi_input], classifier)\n",
    "    total_model = keras.models.Model([img_input, roi_input], rpn[:2] + classifier)\n",
    "\n",
    "    # Load weights\n",
    "    if weights_path != None:\n",
    "        rpn_model.load_weights(weights_path, by_name=True)\n",
    "        classifier_model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "    # Compile models\n",
    "    rpn_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss=[common.rpn_loss_cls(num_anchors, config=config), common.rpn_loss_regr(num_anchors, config=config)])\n",
    "    classifier_model.compile(optimizer=keras.optimizers.Adam(lr=1e-5), loss=[common.class_loss_cls(config=config), common.class_loss_regr(num_classes-1, config=config)], metrics={'dense_class_{}'.format(num_classes): 'accuracy'})\n",
    "    total_model.compile(optimizer='sgd', loss='mae')\n",
    "\n",
    "    # Return models\n",
    "    return rpn_model, classifier_model, total_model\n",
    "\n",
    "# Get inference models\n",
    "def get_inference_models(config=None, num_classes=2, weights_path=None):\n",
    "\n",
    "    # Make sure that there is configurations\n",
    "    if config == None:\n",
    "        config = common.Config()\n",
    "\n",
    "    # Calculate the number of anchors\n",
    "    num_anchors = len(config.anchor_box_scales) * len(config.anchor_box_ratios)\n",
    "\n",
    "    # Create input layers\n",
    "    img_input = keras.layers.Input(shape=(None, None, 3))\n",
    "    roi_input = keras.layers.Input(shape=(config.num_rois, 4))\n",
    "    feature_map_input = keras.layers.Input(shape=(None, None, 512))\n",
    "\n",
    "    # Get a backbone model (VGG here, can be Resnet50, Inception, etc)\n",
    "    backbone = get_vgg_16_model(img_input)\n",
    "\n",
    "    # Create an rpn layer\n",
    "    rpn = layers.rpn_layer(backbone, num_anchors)\n",
    "\n",
    "    # Create a classifier\n",
    "    classifier = layers.classifier_layer(feature_map_input, roi_input, config.num_rois, nb_classes=num_classes)\n",
    "\n",
    "    # Create models\n",
    "    rpn_model = keras.models.Model(img_input, rpn)\n",
    "    classifier_only_model = keras.models.Model([feature_map_input, roi_input], classifier)\n",
    "    classifier_model = keras.models.Model([feature_map_input, roi_input], classifier)\n",
    "\n",
    "    # Load weights\n",
    "    rpn_model.load_weights(weights_path, by_name=True)\n",
    "    classifier_model.load_weights(weights_path, by_name=True)\n",
    "\n",
    "    # Compile models\n",
    "    rpn_model.compile(optimizer='sgd', loss='mse')\n",
    "    classifier_model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "    # Return models\n",
    "    return rpn_model, classifier_model, classifier_only_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Training models is created if no training has been done before, weights can be loaded from a pretrained model. Training models is loaded with saved weights if training has been conducted before, this enables the model to continue training (transfer learning). The result from a run is shown below the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import Chaka_Faster_rcnn.common as commonS\n",
    "import Chaka_Faster_rcnn.Vgg as mb\n",
    "\n",
    "import  Chaka_Faster_rcnn.layers as layers\n",
    "\n",
    "# The main entry point for this module\n",
    "def main():\n",
    "\n",
    "    # Create configuration\n",
    "    config = common.Config()\n",
    "    config.annotations_file_path = 'C:/Users/rzouga/Downloads/Github/CNN_CV/Face_Recognition_Fast_rcnn/TrainFacialRecognitonModel/work/work_V2/Train_annotation.txt'\n",
    "    config.pretrained_model_path = 'C:/Users/rzouga/Downloads/Github/CNN_CV/Face_Recognition_Fast_rcnn/TrainFacialRecognitonModel/work/work_V2/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "    config.model_path = 'C:/Users/rzouga/Downloads/Github/CNN_CV/Face_Recognition_Fast_rcnn/TrainFacialRecognitonModel/work/work_V2/training_model.h5'\n",
    "    config.records_path = 'C:/Users/rzouga/Downloads/Github/CNN_CV/Face_Recognition_Fast_rcnn/TrainFacialRecognitonModel/work/work_V2/records.csv'\n",
    "    config.use_horizontal_flips = True\n",
    "    config.use_vertical_flips = True\n",
    "    config.rot_90 = True\n",
    "    config.num_rois = 4\n",
    "\n",
    "    # Set the gpu to use\n",
    "    common.setup_gpu('cpu')\n",
    "    #common.setup_gpu(0)\n",
    "\n",
    "    # Get data\n",
    "    st = time.time()\n",
    "    images, classes, mappings = common.get_data(config)\n",
    "    print()\n",
    "    print('Spend {0:0.2f} mins to load the data'.format((time.time()-st)/60))\n",
    "\n",
    "    # Make sure that we have a bg class. A background region, this is usually for hard negative mining\n",
    "    if 'bg' not in classes:\n",
    "        classes['bg'] = 0\n",
    "        mappings['bg'] = len(mappings)\n",
    "\n",
    "    # Print class distribution\n",
    "    print('Training images per class:')\n",
    "    print(classes)\n",
    "    print('Number of classes (including bg) = {}'.format(len(classes)))\n",
    "    print(mappings)\n",
    "\n",
    "    # Get a train generator\n",
    "    train_generator = common.get_anchor_gt(images, config, mode='train')\n",
    "\n",
    "    # Get training models\n",
    "    if os.path.isfile(config.model_path):\n",
    "    \n",
    "        # Get training models\n",
    "        model_rpn, model_classifier, model_all = mb.get_training_models(config, len(classes), weights_path=config.model_path)\n",
    "\n",
    "        # Load records\n",
    "        record_df = pd.read_csv(config.records_path)\n",
    "        r_mean_overlapping_bboxes = record_df['mean_overlapping_bboxes']\n",
    "        r_class_acc = record_df['class_acc']\n",
    "        r_loss_rpn_cls = record_df['loss_rpn_cls']\n",
    "        r_loss_rpn_regr = record_df['loss_rpn_regr']\n",
    "        r_loss_class_cls = record_df['loss_class_cls']\n",
    "        r_loss_class_regr = record_df['loss_class_regr']\n",
    "        r_curr_loss = record_df['curr_loss']\n",
    "        r_elapsed_time = record_df['elapsed_time']\n",
    "        r_mAP = record_df['mAP']\n",
    "\n",
    "        print('Already trained {0} batches'.format(len(record_df)))\n",
    "\n",
    "    else:\n",
    "        # Check if we can load weights from pretrained model or not\n",
    "        if os.path.isfile(config.pretrained_model_path):\n",
    "            model_rpn, model_classifier, model_all = mb.get_training_models(config, len(classes), weights_path=config.pretrained_model_path)\n",
    "        else:\n",
    "            model_rpn, model_classifier, model_all = mb.get_training_models(config, len(classes))\n",
    "\n",
    "        # Create a records dataframe\n",
    "        record_df = pd.DataFrame(columns=['mean_overlapping_bboxes', 'class_acc', 'loss_rpn_cls', 'loss_rpn_regr', 'loss_class_cls', 'loss_class_regr', 'curr_loss', 'elapsed_time', 'mAP'])\n",
    "\n",
    "    # Settings for training\n",
    "    total_epochs = len(record_df)\n",
    "    r_epochs = len(record_df)\n",
    "    steps = 240 # 240 images (epoch_length)\n",
    "    num_epochs = 1\n",
    "    iter_num = 0\n",
    "    total_epochs += num_epochs\n",
    "    losses = np.zeros((steps, 5))\n",
    "    rpn_accuracy_rpn_monitor = []\n",
    "    rpn_accuracy_for_epoch = []\n",
    "    if len(record_df)==0:\n",
    "        best_loss = np.Inf\n",
    "    else:\n",
    "        best_loss = np.min(r_curr_loss)\n",
    "\n",
    "    # Start training (one image on each iteration)\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Loop epochs\n",
    "    for epoch_num in range(num_epochs):\n",
    "\n",
    "        # Create a progress bar\n",
    "        progbar = keras.utils.Progbar(steps)\n",
    "\n",
    "        # Print the current epoch\n",
    "        print('Epoch {}/{}'.format(r_epochs + 1, total_epochs))\n",
    "        r_epochs += 1\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "\n",
    "                if len(rpn_accuracy_rpn_monitor) == steps and config.verbose:\n",
    "                    mean_overlapping_bboxes = float(sum(rpn_accuracy_rpn_monitor))/len(rpn_accuracy_rpn_monitor)\n",
    "                    rpn_accuracy_rpn_monitor = []\n",
    "                    if mean_overlapping_bboxes == 0:\n",
    "                        print('RPN is not producing bounding boxes that overlap the ground truth boxes. Check RPN settings or keep training.')\n",
    "\n",
    "                # Get the next element from an generator\n",
    "                # Generate X (x_img) and label Y ([y_rpn_cls, y_rpn_regr])\n",
    "                X, Y, img_data, debug_img, debug_num_pos = next(train_generator)\n",
    "\n",
    "                # Train rpn model and get loss value [_, loss_rpn_cls, loss_rpn_regr]\n",
    "                loss_rpn = model_rpn.train_on_batch(X, Y)\n",
    "\n",
    "                # Get predicted rpn from rpn model [rpn_cls, rpn_regr]\n",
    "                P_rpn = model_rpn.predict_on_batch(X)\n",
    "\n",
    "                # R: bboxes (shape=(300,4))\n",
    "                # Convert rpn layer to roi bboxes\n",
    "                R = layers.rpn_to_roi(P_rpn[0], P_rpn[1], config, keras.backend.image_data_format(), use_regr=True, overlap_thresh=0.7, max_boxes=300)\n",
    "            \n",
    "                # note: calc_iou converts from (x1,y1,x2,y2) to (x,y,w,h) format\n",
    "                # X2: bboxes that iou > config.classifier_min_overlap for all gt bboxes in 300 non_max_suppression bboxes\n",
    "                # Y1: one hot code for bboxes from above => x_roi (X)\n",
    "                # Y2: corresponding labels and corresponding gt bboxes\n",
    "                X2, Y1, Y2, IouS = common.calc_iou(R, img_data, config, mappings)\n",
    "\n",
    "                # If X2 is None means there are no matching bboxes\n",
    "                if X2 is None:\n",
    "                    rpn_accuracy_rpn_monitor.append(0)\n",
    "                    rpn_accuracy_for_epoch.append(0)\n",
    "                    continue\n",
    "            \n",
    "                # Find out the positive anchors and negative anchors\n",
    "                neg_samples = np.where(Y1[0, :, -1] == 1)\n",
    "                pos_samples = np.where(Y1[0, :, -1] == 0)\n",
    "\n",
    "                if len(neg_samples) > 0:\n",
    "                    neg_samples = neg_samples[0]\n",
    "                else:\n",
    "                    neg_samples = []\n",
    "\n",
    "                if len(pos_samples) > 0:\n",
    "                    pos_samples = pos_samples[0]\n",
    "                else:\n",
    "                    pos_samples = []\n",
    "\n",
    "                rpn_accuracy_rpn_monitor.append(len(pos_samples))\n",
    "                rpn_accuracy_for_epoch.append((len(pos_samples)))\n",
    "\n",
    "                if config.num_rois > 1:\n",
    "                    # If number of positive anchors is larger than 4//2 = 2, randomly choose 2 pos samples\n",
    "                    if len(pos_samples) < config.num_rois//2:\n",
    "                        selected_pos_samples = pos_samples.tolist()\n",
    "                    else:\n",
    "                        selected_pos_samples = np.random.choice(pos_samples, config.num_rois//2, replace=False).tolist()\n",
    "                \n",
    "                    # Randomly choose (num_rois - num_pos) neg samples\n",
    "                    try:\n",
    "                        selected_neg_samples = np.random.choice(neg_samples, config.num_rois - len(selected_pos_samples), replace=False).tolist()\n",
    "                    except:\n",
    "                        selected_neg_samples = np.random.choice(neg_samples, config.num_rois - len(selected_pos_samples), replace=True).tolist()\n",
    "                \n",
    "                    # Save all the pos and neg samples in sel_samples\n",
    "                    sel_samples = selected_pos_samples + selected_neg_samples\n",
    "                else:\n",
    "                    # in the extreme case where num_rois = 1, we pick a random pos or neg sample\n",
    "                    selected_pos_samples = pos_samples.tolist()\n",
    "                    selected_neg_samples = neg_samples.tolist()\n",
    "                    if np.random.randint(0, 2):\n",
    "                        sel_samples = random.choice(neg_samples)\n",
    "                    else:\n",
    "                        sel_samples = random.choice(pos_samples)\n",
    "\n",
    "                # Train classifier\n",
    "                loss_class = model_classifier.train_on_batch([X, X2[:, sel_samples, :]], [Y1[:, sel_samples, :], Y2[:, sel_samples, :]])\n",
    "                losses[iter_num, 0] = loss_rpn[1]\n",
    "                losses[iter_num, 1] = loss_rpn[2]\n",
    "                losses[iter_num, 2] = loss_class[1]\n",
    "                losses[iter_num, 3] = loss_class[2]\n",
    "                losses[iter_num, 4] = loss_class[3]\n",
    "                iter_num += 1\n",
    "\n",
    "                # Update the progress bar\n",
    "                progbar.update(iter_num, [('rpn_cls', np.mean(losses[:iter_num, 0])), ('rpn_regr', np.mean(losses[:iter_num, 1])),\n",
    "                                            ('final_cls', np.mean(losses[:iter_num, 2])), ('final_regr', np.mean(losses[:iter_num, 3]))])\n",
    "            \n",
    "                # Free memory before each step\n",
    "                #tf.keras.backend.clear_session()\n",
    "\n",
    "                # Start the next step\n",
    "                if iter_num == steps:\n",
    "                    loss_rpn_cls = np.mean(losses[:, 0])\n",
    "                    loss_rpn_regr = np.mean(losses[:, 1])\n",
    "                    loss_class_cls = np.mean(losses[:, 2])\n",
    "                    loss_class_regr = np.mean(losses[:, 3])\n",
    "                    class_acc = np.mean(losses[:, 4])\n",
    "\n",
    "                    mean_overlapping_bboxes = float(sum(rpn_accuracy_for_epoch)) / len(rpn_accuracy_for_epoch)\n",
    "                    rpn_accuracy_for_epoch = []\n",
    "\n",
    "                    if config.verbose:\n",
    "                        print('Mean number of bounding boxes from RPN overlapping ground truth boxes: {0}'.format(mean_overlapping_bboxes))\n",
    "                        print('Classifier accuracy for bounding boxes from RPN: {0}'.format(class_acc))\n",
    "                        print('Loss RPN classifier: {0}'.format(loss_rpn_cls))\n",
    "                        print('Loss RPN regression: {0}'.format(loss_rpn_regr))\n",
    "                        print('Loss Detector classifier: {0}'.format(loss_class_cls))\n",
    "                        print('Loss Detector regression: {0}'.format(loss_class_regr))\n",
    "                        print('Total loss: {0}'.format(loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr))\n",
    "                        print('Elapsed time: {0}'.format(time.time() - start_time))\n",
    "                        elapsed_time = (time.time()-start_time)/60\n",
    "\n",
    "                    curr_loss = loss_rpn_cls + loss_rpn_regr + loss_class_cls + loss_class_regr\n",
    "                    iter_num = 0\n",
    "                    start_time = time.time()\n",
    "\n",
    "                    # Save the model\n",
    "                    if curr_loss < best_loss:\n",
    "                        if config.verbose:\n",
    "                            print('Total loss decreased from {0} to {1}, saving model.'.format(best_loss,curr_loss))\n",
    "                        best_loss = curr_loss\n",
    "                        #model_all.save_weights(config.model_path)\n",
    "                        model_all.save(config.model_path)\n",
    "\n",
    "                    # Create a new records row\n",
    "                    new_row = {'mean_overlapping_bboxes':round(mean_overlapping_bboxes, 3), \n",
    "                                'class_acc':round(class_acc, 3), \n",
    "                                'loss_rpn_cls':round(loss_rpn_cls, 3), \n",
    "                                'loss_rpn_regr':round(loss_rpn_regr, 3), \n",
    "                                'loss_class_cls':round(loss_class_cls, 3), \n",
    "                                'loss_class_regr':round(loss_class_regr, 3), \n",
    "                                'curr_loss':round(curr_loss, 3), \n",
    "                                'elapsed_time':round(elapsed_time, 3), \n",
    "                                'mAP': 0}\n",
    "\n",
    "                    # Append a new record row and save the file\n",
    "                    record_df = record_df.append(new_row, ignore_index=True)\n",
    "                    record_df.to_csv(config.records_path, index=0)\n",
    "\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                print('Exception: {}'.format(e))\n",
    "                continue\n",
    "\n",
    "        # Free memory after each epoch\n",
    "        #tf.keras.backend.clear_session()\n",
    "\n",
    "    print('Training complete, exiting.')\n",
    "\n",
    "# Tell python to run main method\n",
    "if __name__ == \"__main__\": main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "This module is used to perform visual evaluation of the model. I have only trained the model in 21 epochs, this type of models needs a lot of training to perform well. The result from an evaluation is shown below the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import Chaka_Faster_rcnn.common as common\n",
    "import Chaka_Faster_rcnn.Vgg as mb\n",
    "\n",
    "import  Chaka_Faster_rcnn.layers as layers\n",
    "\n",
    "# The main entry point for this module\n",
    "def main():\n",
    "\n",
    "    # Create configuration\n",
    "    config = common.Config()\n",
    "    config.model_path = 'C:/Users/rzouga/Downloads/Github/CNN_CV/Face_Recognition_Fast_rcnn/TrainFacialRecognitonModel/work/work_V2/training_model.h5'\n",
    "    config.use_horizontal_flips = False\n",
    "    config.use_vertical_flips = False\n",
    "    config.rot_90 = False\n",
    "    config.num_rois = 4\n",
    "\n",
    "    # Set the gpu to use\n",
    "    common.setup_gpu('cpu')\n",
    "    #common.setup_gpu(0)\n",
    "\n",
    "    # Create a dictionary with classes and switch key values\n",
    "    classes = {'Noface': 0, 'face': 1}\n",
    "    mappings = {v: k for k, v in classes.items()}\n",
    "    class_to_color = {mappings[v]: np.random.randint(0, 255, 3) for v in mappings}\n",
    "    print(mappings)\n",
    "\n",
    "    # Get inference models\n",
    "    model_rpn, model_classifier, model_classifier_only = mb.get_inference_models(config, len(classes), weights_path=config.model_path)\n",
    "\n",
    "    # Get a list of test images\n",
    "    images = os.listdir('C:/Users/rzouga/Downloads/Github/CNN_CV/Face_Recognition_Fast_rcnn/TrainFacialRecognitonModel/wiki_crop/wiki_crop/01/')\n",
    "\n",
    "    # Randomize images to get different images each time\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # If the box classification value is less than this, we ignore this box\n",
    "\t#must return to 0.7\n",
    "    bbox_threshold = 0.7\n",
    "\n",
    "    # Start visual evaluation\n",
    "    for i, name in enumerate(images):\n",
    "\n",
    "        # Limit the evaluation to 4 images\n",
    "        if i > 4:\n",
    "            break;\n",
    "\n",
    "        # Print the name and start a timer\n",
    "        print(name)\n",
    "        st = time.time()\n",
    "\n",
    "        # Get the filepath\n",
    "        filepath = os.path.join('C:/Users/rzouga/Downloads/Github/CNN_CV/Face_Recognition_Fast_rcnn/TrainFacialRecognitonModel/wiki_crop/wiki_crop/01/', name)\n",
    "\n",
    "        # Get the image\n",
    "        img = cv2.imread(filepath)\n",
    "\n",
    "        # Format the image\n",
    "        X, ratio = common.format_img(img, config)\n",
    "        X = np.transpose(X, (0, 2, 3, 1))\n",
    "\n",
    "        # Get output layer Y1, Y2 from the RPN and the feature maps F\n",
    "        # Y1: y_rpn_cls\n",
    "        # Y2: y_rpn_regr\n",
    "        [Y1, Y2, F] = model_rpn.predict(X)\n",
    "\n",
    "        # Get bboxes by applying NMS \n",
    "        # R.shape = (300, 4)\n",
    "        R = layers.rpn_to_roi(Y1, Y2, config, keras.backend.image_data_format(), overlap_thresh=0.7)\n",
    "\n",
    "        # Convert from (x1,y1,x2,y2) to (x,y,w,h)\n",
    "        R[:, 2] -= R[:, 0]\n",
    "        R[:, 3] -= R[:, 1]\n",
    "\n",
    "        # Apply the spatial pyramid pooling to the proposed regions\n",
    "        bboxes = {}\n",
    "        probs = {}\n",
    "\n",
    "        for jk in range(R.shape[0]//config.num_rois + 1):\n",
    "            ROIs = np.expand_dims(R[config.num_rois*jk:config.num_rois*(jk+1), :], axis=0)\n",
    "            if ROIs.shape[1] == 0:\n",
    "                break\n",
    "\n",
    "            if jk == R.shape[0]//config.num_rois:\n",
    "                #pad R\n",
    "                curr_shape = ROIs.shape\n",
    "                target_shape = (curr_shape[0],config.num_rois,curr_shape[2])\n",
    "                ROIs_padded = np.zeros(target_shape).astype(ROIs.dtype)\n",
    "                ROIs_padded[:, :curr_shape[1], :] = ROIs\n",
    "                ROIs_padded[0, curr_shape[1]:, :] = ROIs[0, 0, :]\n",
    "                ROIs = ROIs_padded\n",
    "\n",
    "            [P_cls, P_regr] = model_classifier_only.predict([F, ROIs])\n",
    "\n",
    "            # Calculate bboxes coordinates on resized image\n",
    "            for ii in range(P_cls.shape[1]):\n",
    "                # Ignore 'bg' class\n",
    "                if np.max(P_cls[0, ii, :]) < bbox_threshold or np.argmax(P_cls[0, ii, :]) == (P_cls.shape[2] - 1):\n",
    "                    continue\n",
    "\n",
    "                # Get the class name\n",
    "                cls_name = mappings[np.argmax(P_cls[0, ii, :])]\n",
    "\n",
    "                if cls_name not in bboxes:\n",
    "                    bboxes[cls_name] = []\n",
    "                    probs[cls_name] = []\n",
    "\n",
    "                (x, y, w, h) = ROIs[0, ii, :]\n",
    "\n",
    "                cls_num = np.argmax(P_cls[0, ii, :])\n",
    "                try:\n",
    "                    (tx, ty, tw, th) = P_regr[0, ii, 4*cls_num:4*(cls_num+1)]\n",
    "                    tx /= C.classifier_regr_std[0]\n",
    "                    ty /= C.classifier_regr_std[1]\n",
    "                    tw /= C.classifier_regr_std[2]\n",
    "                    th /= C.classifier_regr_std[3]\n",
    "                    x, y, w, h = layers.apply_regr(x, y, w, h, tx, ty, tw, th)\n",
    "                except:\n",
    "                    pass\n",
    "                bboxes[cls_name].append([config.rpn_stride*x, config.rpn_stride*y, config.rpn_stride*(x+w), config.rpn_stride*(y+h)])\n",
    "                probs[cls_name].append(np.max(P_cls[0, ii, :]))\n",
    "\n",
    "        all_dets = []\n",
    "\n",
    "        for key in bboxes:\n",
    "            bbox = np.array(bboxes[key])\n",
    "\n",
    "            new_boxes, new_probs = common.non_max_suppression_fast(bbox, np.array(probs[key]), overlap_thresh=0.2)\n",
    "            for jk in range(new_boxes.shape[0]):\n",
    "                (x1, y1, x2, y2) = new_boxes[jk,:]\n",
    "\n",
    "                # Calculate real coordinates on original image\n",
    "                (real_x1, real_y1, real_x2, real_y2) = common.get_real_coordinates(ratio, x1, y1, x2, y2)\n",
    "\n",
    "                cv2.rectangle(img,(real_x1, real_y1), (real_x2, real_y2), (int(class_to_color[key][0]), int(class_to_color[key][1]), int(class_to_color[key][2])),4)\n",
    "\n",
    "                textLabel = '{0}: {1}'.format(key,int(100*new_probs[jk]))\n",
    "                all_dets.append((key,100*new_probs[jk]))\n",
    "\n",
    "                (retval,baseLine) = cv2.getTextSize(textLabel,cv2.FONT_HERSHEY_COMPLEX,1,1)\n",
    "                textOrg = (real_x1, real_y1-0)\n",
    "\n",
    "                cv2.rectangle(img, (textOrg[0] - 5, textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (0, 0, 0), 1)\n",
    "                cv2.rectangle(img, (textOrg[0] - 5,textOrg[1]+baseLine - 5), (textOrg[0]+retval[0] + 5, textOrg[1]-retval[1] - 5), (255, 255, 255), -1)\n",
    "                cv2.putText(img, textLabel, textOrg, cv2.FONT_HERSHEY_DUPLEX, 1, (0, 0, 0), 1)\n",
    "\n",
    "        print('Elapsed time = {0}'.format(time.time() - st))\n",
    "        print(all_dets)\n",
    "        plt.figure(figsize=(10,10))\n",
    "        plt.grid()\n",
    "        plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))\n",
    "        plt.savefig('C:/Users/rzouga/Downloads/Github/CNN_CV/Face_Recognition_Fast_rcnn/TrainFacialRecognitonModel/work/work_V2/Test_Result/' + str(i) + '.png')\n",
    "        #plt.show()\n",
    "\n",
    "# Tell python to run main method\n",
    "if __name__ == \"__main__\": main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
